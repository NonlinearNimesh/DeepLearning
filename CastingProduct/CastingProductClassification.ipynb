{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CastingProductClassification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYKKKyS8fMeF"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "159kACt5YZ9P"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 \n",
        "from tensorflow.python.keras.applications import densenet\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "from tensorflow.keras.applications import Xception\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-w_9ZavfVNZ"
      },
      "source": [
        "#Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky1j5le_faFx"
      },
      "source": [
        "This work mainly based on transfer learning where i have used the pre-trained models trained on imagenet except final layer so that it can train our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIVNQK3vfZUv",
        "outputId": "8879b7ab-1362-4411-a1b1-d5e13934994b"
      },
      "source": [
        "transferLearningModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "transferModel = transferLearningModel.output\n",
        "transferModel = Flatten(name=\"flatten\")(transferModel)\n",
        "transferModel = Dense(1024, activation='relu')(transferModel)\n",
        "transferModel = Dropout(0.5)(transferModel)\n",
        "transferModel = Dense(1, activation='sigmoid')(transferModel)\n",
        "\n",
        "model = Model(inputs=transferLearningModel.input, outputs=transferModel)\n",
        "for layer in transferLearningModel.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] loading imagenet weights...\n",
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni61BnRBgLiA"
      },
      "source": [
        "Next, we instantiate an object named train_datagen and test_datagen from theImageDataGenerator class, specifying that the pixels will be normalized to the 0-1 range (by dividing by 255)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbhd0_kyfQqY"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMAEo-2Qglt7"
      },
      "source": [
        "Further, Set the directory for going through our data along with that we define standard batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxkD6Jylgd5F"
      },
      "source": [
        "batch_size = 32\n",
        "training_directory = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/train\"     #directory of training data\n",
        "test_directory = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/test\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkno22lxhq4z"
      },
      "source": [
        "Then, we use the flow_from_directory method to effectively create the objects that will generate the training and validation minibatches. \n",
        "Image size we have define the standard on i.e. (224,224) which is generally an input shape for most of the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94FKbC4g5qk"
      },
      "source": [
        "train_set = train_datagen.flow_from_directory(training_directory, \n",
        "                                                 target_size=(224, 224),\n",
        "                                                 batch_size=batch_size, \n",
        "                                                 class_mode='binary')\n",
        "test_set = test_datagen.flow_from_directory(test_directory, \n",
        "                                            target_size=(224, 224),\n",
        "                                            batch_size=batch_size, \n",
        "                                            class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeJte3-HjOcc"
      },
      "source": [
        "Next, some of the parameter is defined. where learning rate i a tested manually from 0.01 to 0.00001. And after running continously i came down to this 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsNkPtxIg-fV"
      },
      "source": [
        "optimizer = Adam(lr=0.0001)\n",
        "epoch = 10\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCVGx3mthPj6"
      },
      "source": [
        "history = model.fit_generator(train_set,\n",
        "                        steps_per_epoch=train_set.samples//batch_size,\n",
        "                        validation_data=test_set,\n",
        "                        epochs=epoch,\n",
        "                        validation_steps=test_set.samples//batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhVZdtmsjclt"
      },
      "source": [
        "Next we plot the graph for visualization of accuracy and loss to obseve the perfomance and thus i come to conclusion that 10 spoch is enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voOtO5nyhRtY"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train_acc', 'val_acc'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg2G_T7thR3s"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEhirbm4jv3M"
      },
      "source": [
        "# Testing our model with some images from test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkaCvejuheI4"
      },
      "source": [
        "import cv2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "%matplotlib inline\n",
        "imagepath = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/test/def_front/cast_def_0_1191.jpeg\"\n",
        "img = cv2.imread(imagepath)\n",
        "img = cv2.resize(img, (224, 224))\n",
        "orig = img.copy()\n",
        "img = image.img_to_array(img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "img = img/255\n",
        "\n",
        "print(\"[Info] predicting output\")\n",
        "prediction = model.predict(img)\n",
        "if (prediction<0.5):\n",
        "    print(\"def_front\")\n",
        "    cv2.putText(orig, \"def_front\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "else:\n",
        "    print(\"ok_front\")\n",
        "    cv2.putText(orig, \"ok_front\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "    \n",
        "plt.imshow(orig)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nd4LyL3hfR9"
      },
      "source": [
        "%matplotlib inline\n",
        "imagepath = \"../input/real-life-industrial-dataset-of-casting-product/casting_data/casting_data/test/ok_front/cast_ok_0_1092.jpeg\"\n",
        "img = cv2.imread(imagepath)\n",
        "img = cv2.resize(img, (224, 224))\n",
        "orig = img.copy()\n",
        "img = image.img_to_array(img)\n",
        "img = np.expand_dims(img, axis=0)\n",
        "img = img/255\n",
        "\n",
        "print(\"[Info] predicting output\")\n",
        "prediction = model.predict(img)\n",
        "if (prediction<0.5):\n",
        "    print(\"def_front\")\n",
        "    cv2.putText(orig, \"def_front\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "else:\n",
        "    print(\"ok_front\")\n",
        "    cv2.putText(orig, \"ok_front\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "    \n",
        "plt.imshow(orig)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqN1PjD7j3f0"
      },
      "source": [
        "# ....................................................................................."
      ]
    }
  ]
}